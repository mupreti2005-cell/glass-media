{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3321,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009033423667570008,
      "grad_norm": 5.929014205932617,
      "learning_rate": 1.9939777175549535e-05,
      "loss": 0.6831,
      "step": 10
    },
    {
      "epoch": 0.018066847335140017,
      "grad_norm": 5.086696147918701,
      "learning_rate": 1.987955435109907e-05,
      "loss": 0.6883,
      "step": 20
    },
    {
      "epoch": 0.02710027100271003,
      "grad_norm": 11.147159576416016,
      "learning_rate": 1.9819331526648603e-05,
      "loss": 0.7055,
      "step": 30
    },
    {
      "epoch": 0.036133694670280034,
      "grad_norm": 6.254363059997559,
      "learning_rate": 1.9759108702198137e-05,
      "loss": 0.6601,
      "step": 40
    },
    {
      "epoch": 0.045167118337850046,
      "grad_norm": 8.51333236694336,
      "learning_rate": 1.9698885877747667e-05,
      "loss": 0.5979,
      "step": 50
    },
    {
      "epoch": 0.05420054200542006,
      "grad_norm": 10.468064308166504,
      "learning_rate": 1.96386630532972e-05,
      "loss": 0.5861,
      "step": 60
    },
    {
      "epoch": 0.06323396567299007,
      "grad_norm": 10.069010734558105,
      "learning_rate": 1.9578440228846735e-05,
      "loss": 0.6045,
      "step": 70
    },
    {
      "epoch": 0.07226738934056007,
      "grad_norm": 6.80264139175415,
      "learning_rate": 1.9518217404396266e-05,
      "loss": 0.5629,
      "step": 80
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 7.924543380737305,
      "learning_rate": 1.94579945799458e-05,
      "loss": 0.5962,
      "step": 90
    },
    {
      "epoch": 0.09033423667570009,
      "grad_norm": 10.897093772888184,
      "learning_rate": 1.9397771755495333e-05,
      "loss": 0.5034,
      "step": 100
    },
    {
      "epoch": 0.0993676603432701,
      "grad_norm": 8.67173957824707,
      "learning_rate": 1.9337548931044867e-05,
      "loss": 0.5828,
      "step": 110
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 8.36992073059082,
      "learning_rate": 1.92773261065944e-05,
      "loss": 0.4928,
      "step": 120
    },
    {
      "epoch": 0.11743450767841011,
      "grad_norm": 11.74995231628418,
      "learning_rate": 1.9217103282143935e-05,
      "loss": 0.5115,
      "step": 130
    },
    {
      "epoch": 0.12646793134598014,
      "grad_norm": 14.124268531799316,
      "learning_rate": 1.915688045769347e-05,
      "loss": 0.551,
      "step": 140
    },
    {
      "epoch": 0.13550135501355012,
      "grad_norm": 15.641669273376465,
      "learning_rate": 1.9096657633243002e-05,
      "loss": 0.4866,
      "step": 150
    },
    {
      "epoch": 0.14453477868112014,
      "grad_norm": 10.611469268798828,
      "learning_rate": 1.9036434808792533e-05,
      "loss": 0.5273,
      "step": 160
    },
    {
      "epoch": 0.15356820234869015,
      "grad_norm": 7.717434406280518,
      "learning_rate": 1.8976211984342067e-05,
      "loss": 0.4581,
      "step": 170
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 9.515811920166016,
      "learning_rate": 1.89159891598916e-05,
      "loss": 0.5335,
      "step": 180
    },
    {
      "epoch": 0.17163504968383017,
      "grad_norm": 10.10418701171875,
      "learning_rate": 1.8855766335441134e-05,
      "loss": 0.4801,
      "step": 190
    },
    {
      "epoch": 0.18066847335140018,
      "grad_norm": 8.626113891601562,
      "learning_rate": 1.8795543510990668e-05,
      "loss": 0.6163,
      "step": 200
    },
    {
      "epoch": 0.1897018970189702,
      "grad_norm": 10.279836654663086,
      "learning_rate": 1.87353206865402e-05,
      "loss": 0.4417,
      "step": 210
    },
    {
      "epoch": 0.1987353206865402,
      "grad_norm": 6.876883029937744,
      "learning_rate": 1.8675097862089732e-05,
      "loss": 0.5147,
      "step": 220
    },
    {
      "epoch": 0.20776874435411022,
      "grad_norm": 11.59598159790039,
      "learning_rate": 1.8614875037639266e-05,
      "loss": 0.6074,
      "step": 230
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 5.525062561035156,
      "learning_rate": 1.85546522131888e-05,
      "loss": 0.5228,
      "step": 240
    },
    {
      "epoch": 0.22583559168925021,
      "grad_norm": 7.126251697540283,
      "learning_rate": 1.8494429388738334e-05,
      "loss": 0.4573,
      "step": 250
    },
    {
      "epoch": 0.23486901535682023,
      "grad_norm": 10.859749794006348,
      "learning_rate": 1.8434206564287868e-05,
      "loss": 0.5233,
      "step": 260
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 7.888726711273193,
      "learning_rate": 1.83739837398374e-05,
      "loss": 0.5491,
      "step": 270
    },
    {
      "epoch": 0.2529358626919603,
      "grad_norm": 6.1198296546936035,
      "learning_rate": 1.8313760915386932e-05,
      "loss": 0.5927,
      "step": 280
    },
    {
      "epoch": 0.26196928635953026,
      "grad_norm": 10.330803871154785,
      "learning_rate": 1.8253538090936466e-05,
      "loss": 0.5279,
      "step": 290
    },
    {
      "epoch": 0.27100271002710025,
      "grad_norm": 5.128421783447266,
      "learning_rate": 1.8193315266486e-05,
      "loss": 0.542,
      "step": 300
    },
    {
      "epoch": 0.2800361336946703,
      "grad_norm": 7.392498970031738,
      "learning_rate": 1.8133092442035533e-05,
      "loss": 0.5345,
      "step": 310
    },
    {
      "epoch": 0.28906955736224027,
      "grad_norm": 7.79334020614624,
      "learning_rate": 1.8072869617585067e-05,
      "loss": 0.481,
      "step": 320
    },
    {
      "epoch": 0.2981029810298103,
      "grad_norm": 12.469452857971191,
      "learning_rate": 1.80126467931346e-05,
      "loss": 0.5801,
      "step": 330
    },
    {
      "epoch": 0.3071364046973803,
      "grad_norm": 6.438068389892578,
      "learning_rate": 1.795242396868413e-05,
      "loss": 0.5643,
      "step": 340
    },
    {
      "epoch": 0.31616982836495033,
      "grad_norm": 11.485504150390625,
      "learning_rate": 1.7892201144233665e-05,
      "loss": 0.4913,
      "step": 350
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 8.613139152526855,
      "learning_rate": 1.78319783197832e-05,
      "loss": 0.6624,
      "step": 360
    },
    {
      "epoch": 0.33423667570009036,
      "grad_norm": 6.343311309814453,
      "learning_rate": 1.7771755495332733e-05,
      "loss": 0.5233,
      "step": 370
    },
    {
      "epoch": 0.34327009936766034,
      "grad_norm": 6.741794109344482,
      "learning_rate": 1.7711532670882267e-05,
      "loss": 0.4289,
      "step": 380
    },
    {
      "epoch": 0.3523035230352303,
      "grad_norm": 6.751738548278809,
      "learning_rate": 1.7651309846431797e-05,
      "loss": 0.4561,
      "step": 390
    },
    {
      "epoch": 0.36133694670280037,
      "grad_norm": 5.281815528869629,
      "learning_rate": 1.759108702198133e-05,
      "loss": 0.509,
      "step": 400
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 7.364655017852783,
      "learning_rate": 1.7530864197530865e-05,
      "loss": 0.4572,
      "step": 410
    },
    {
      "epoch": 0.3794037940379404,
      "grad_norm": 8.143705368041992,
      "learning_rate": 1.74706413730804e-05,
      "loss": 0.5698,
      "step": 420
    },
    {
      "epoch": 0.3884372177055104,
      "grad_norm": 9.055045127868652,
      "learning_rate": 1.7410418548629933e-05,
      "loss": 0.4876,
      "step": 430
    },
    {
      "epoch": 0.3974706413730804,
      "grad_norm": 6.950096607208252,
      "learning_rate": 1.7350195724179466e-05,
      "loss": 0.4079,
      "step": 440
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 9.424732208251953,
      "learning_rate": 1.7289972899729e-05,
      "loss": 0.5091,
      "step": 450
    },
    {
      "epoch": 0.41553748870822044,
      "grad_norm": 8.84267807006836,
      "learning_rate": 1.7229750075278534e-05,
      "loss": 0.4776,
      "step": 460
    },
    {
      "epoch": 0.4245709123757904,
      "grad_norm": 7.801748275756836,
      "learning_rate": 1.7169527250828065e-05,
      "loss": 0.4615,
      "step": 470
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 6.092129707336426,
      "learning_rate": 1.71093044263776e-05,
      "loss": 0.4021,
      "step": 480
    },
    {
      "epoch": 0.44263775971093045,
      "grad_norm": 5.660452842712402,
      "learning_rate": 1.7049081601927132e-05,
      "loss": 0.514,
      "step": 490
    },
    {
      "epoch": 0.45167118337850043,
      "grad_norm": 6.8261003494262695,
      "learning_rate": 1.6988858777476666e-05,
      "loss": 0.5826,
      "step": 500
    },
    {
      "epoch": 0.46070460704607047,
      "grad_norm": 5.258693695068359,
      "learning_rate": 1.6928635953026196e-05,
      "loss": 0.4959,
      "step": 510
    },
    {
      "epoch": 0.46973803071364045,
      "grad_norm": 4.526933193206787,
      "learning_rate": 1.686841312857573e-05,
      "loss": 0.4673,
      "step": 520
    },
    {
      "epoch": 0.4787714543812105,
      "grad_norm": 5.800321578979492,
      "learning_rate": 1.6808190304125264e-05,
      "loss": 0.4651,
      "step": 530
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 6.4023356437683105,
      "learning_rate": 1.6747967479674798e-05,
      "loss": 0.535,
      "step": 540
    },
    {
      "epoch": 0.4968383017163505,
      "grad_norm": 5.827235221862793,
      "learning_rate": 1.6687744655224332e-05,
      "loss": 0.4651,
      "step": 550
    },
    {
      "epoch": 0.5058717253839206,
      "grad_norm": 6.478046894073486,
      "learning_rate": 1.6627521830773866e-05,
      "loss": 0.5061,
      "step": 560
    },
    {
      "epoch": 0.5149051490514905,
      "grad_norm": 6.2189249992370605,
      "learning_rate": 1.65672990063234e-05,
      "loss": 0.4486,
      "step": 570
    },
    {
      "epoch": 0.5239385727190605,
      "grad_norm": 6.122732162475586,
      "learning_rate": 1.6507076181872933e-05,
      "loss": 0.5367,
      "step": 580
    },
    {
      "epoch": 0.5329719963866305,
      "grad_norm": 3.647125244140625,
      "learning_rate": 1.6446853357422464e-05,
      "loss": 0.4424,
      "step": 590
    },
    {
      "epoch": 0.5420054200542005,
      "grad_norm": 6.661576747894287,
      "learning_rate": 1.6386630532971998e-05,
      "loss": 0.4047,
      "step": 600
    },
    {
      "epoch": 0.5510388437217706,
      "grad_norm": 16.844118118286133,
      "learning_rate": 1.632640770852153e-05,
      "loss": 0.4104,
      "step": 610
    },
    {
      "epoch": 0.5600722673893406,
      "grad_norm": 5.775938987731934,
      "learning_rate": 1.6266184884071062e-05,
      "loss": 0.4847,
      "step": 620
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 9.897987365722656,
      "learning_rate": 1.6205962059620596e-05,
      "loss": 0.4707,
      "step": 630
    },
    {
      "epoch": 0.5781391147244805,
      "grad_norm": 10.743780136108398,
      "learning_rate": 1.614573923517013e-05,
      "loss": 0.4649,
      "step": 640
    },
    {
      "epoch": 0.5871725383920506,
      "grad_norm": 7.622700214385986,
      "learning_rate": 1.6085516410719663e-05,
      "loss": 0.4803,
      "step": 650
    },
    {
      "epoch": 0.5962059620596206,
      "grad_norm": 7.238577365875244,
      "learning_rate": 1.6025293586269197e-05,
      "loss": 0.3972,
      "step": 660
    },
    {
      "epoch": 0.6052393857271906,
      "grad_norm": 9.119075775146484,
      "learning_rate": 1.596507076181873e-05,
      "loss": 0.4318,
      "step": 670
    },
    {
      "epoch": 0.6142728093947606,
      "grad_norm": 10.843573570251465,
      "learning_rate": 1.5904847937368265e-05,
      "loss": 0.465,
      "step": 680
    },
    {
      "epoch": 0.6233062330623306,
      "grad_norm": 12.383638381958008,
      "learning_rate": 1.58446251129178e-05,
      "loss": 0.5098,
      "step": 690
    },
    {
      "epoch": 0.6323396567299007,
      "grad_norm": 10.583459854125977,
      "learning_rate": 1.5784402288467332e-05,
      "loss": 0.4496,
      "step": 700
    },
    {
      "epoch": 0.6413730803974707,
      "grad_norm": 7.655613899230957,
      "learning_rate": 1.5724179464016863e-05,
      "loss": 0.55,
      "step": 710
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 4.991256237030029,
      "learning_rate": 1.5663956639566397e-05,
      "loss": 0.5517,
      "step": 720
    },
    {
      "epoch": 0.6594399277326106,
      "grad_norm": 5.728504657745361,
      "learning_rate": 1.560373381511593e-05,
      "loss": 0.4877,
      "step": 730
    },
    {
      "epoch": 0.6684733514001807,
      "grad_norm": 5.084813594818115,
      "learning_rate": 1.554351099066546e-05,
      "loss": 0.3769,
      "step": 740
    },
    {
      "epoch": 0.6775067750677507,
      "grad_norm": 6.165623188018799,
      "learning_rate": 1.5483288166214995e-05,
      "loss": 0.4475,
      "step": 750
    },
    {
      "epoch": 0.6865401987353207,
      "grad_norm": 8.384305000305176,
      "learning_rate": 1.542306534176453e-05,
      "loss": 0.6114,
      "step": 760
    },
    {
      "epoch": 0.6955736224028907,
      "grad_norm": 5.649368762969971,
      "learning_rate": 1.5362842517314063e-05,
      "loss": 0.4728,
      "step": 770
    },
    {
      "epoch": 0.7046070460704607,
      "grad_norm": 6.247336387634277,
      "learning_rate": 1.5302619692863596e-05,
      "loss": 0.4329,
      "step": 780
    },
    {
      "epoch": 0.7136404697380307,
      "grad_norm": 5.186068058013916,
      "learning_rate": 1.524239686841313e-05,
      "loss": 0.4658,
      "step": 790
    },
    {
      "epoch": 0.7226738934056007,
      "grad_norm": 3.732757329940796,
      "learning_rate": 1.5182174043962662e-05,
      "loss": 0.5309,
      "step": 800
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 6.9326910972595215,
      "learning_rate": 1.5121951219512196e-05,
      "loss": 0.57,
      "step": 810
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 5.661649703979492,
      "learning_rate": 1.506172839506173e-05,
      "loss": 0.4246,
      "step": 820
    },
    {
      "epoch": 0.7497741644083108,
      "grad_norm": 11.37024211883545,
      "learning_rate": 1.5001505570611264e-05,
      "loss": 0.4578,
      "step": 830
    },
    {
      "epoch": 0.7588075880758808,
      "grad_norm": 6.073853969573975,
      "learning_rate": 1.4941282746160796e-05,
      "loss": 0.4371,
      "step": 840
    },
    {
      "epoch": 0.7678410117434508,
      "grad_norm": 10.572752952575684,
      "learning_rate": 1.488105992171033e-05,
      "loss": 0.5338,
      "step": 850
    },
    {
      "epoch": 0.7768744354110207,
      "grad_norm": 3.8050224781036377,
      "learning_rate": 1.4820837097259864e-05,
      "loss": 0.4416,
      "step": 860
    },
    {
      "epoch": 0.7859078590785907,
      "grad_norm": 5.743560314178467,
      "learning_rate": 1.4760614272809397e-05,
      "loss": 0.4611,
      "step": 870
    },
    {
      "epoch": 0.7949412827461608,
      "grad_norm": 3.7191431522369385,
      "learning_rate": 1.4700391448358928e-05,
      "loss": 0.4863,
      "step": 880
    },
    {
      "epoch": 0.8039747064137308,
      "grad_norm": 8.330262184143066,
      "learning_rate": 1.4640168623908462e-05,
      "loss": 0.5598,
      "step": 890
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 3.629903793334961,
      "learning_rate": 1.4579945799457996e-05,
      "loss": 0.4337,
      "step": 900
    },
    {
      "epoch": 0.8220415537488708,
      "grad_norm": 16.70210838317871,
      "learning_rate": 1.451972297500753e-05,
      "loss": 0.3844,
      "step": 910
    },
    {
      "epoch": 0.8310749774164409,
      "grad_norm": 2.268352746963501,
      "learning_rate": 1.4459500150557062e-05,
      "loss": 0.4227,
      "step": 920
    },
    {
      "epoch": 0.8401084010840109,
      "grad_norm": 6.177801609039307,
      "learning_rate": 1.4399277326106595e-05,
      "loss": 0.3714,
      "step": 930
    },
    {
      "epoch": 0.8491418247515808,
      "grad_norm": 14.750059127807617,
      "learning_rate": 1.433905450165613e-05,
      "loss": 0.4833,
      "step": 940
    },
    {
      "epoch": 0.8581752484191508,
      "grad_norm": 5.707186698913574,
      "learning_rate": 1.4278831677205663e-05,
      "loss": 0.5098,
      "step": 950
    },
    {
      "epoch": 0.8672086720867209,
      "grad_norm": 6.532968044281006,
      "learning_rate": 1.4218608852755195e-05,
      "loss": 0.4785,
      "step": 960
    },
    {
      "epoch": 0.8762420957542909,
      "grad_norm": 5.240503311157227,
      "learning_rate": 1.4158386028304729e-05,
      "loss": 0.3463,
      "step": 970
    },
    {
      "epoch": 0.8852755194218609,
      "grad_norm": 12.33615779876709,
      "learning_rate": 1.4098163203854263e-05,
      "loss": 0.3568,
      "step": 980
    },
    {
      "epoch": 0.8943089430894309,
      "grad_norm": 1.7933276891708374,
      "learning_rate": 1.4037940379403797e-05,
      "loss": 0.4368,
      "step": 990
    },
    {
      "epoch": 0.9033423667570009,
      "grad_norm": 7.832179546356201,
      "learning_rate": 1.3977717554953329e-05,
      "loss": 0.379,
      "step": 1000
    },
    {
      "epoch": 0.912375790424571,
      "grad_norm": 8.418477058410645,
      "learning_rate": 1.3917494730502861e-05,
      "loss": 0.481,
      "step": 1010
    },
    {
      "epoch": 0.9214092140921409,
      "grad_norm": 6.553675174713135,
      "learning_rate": 1.3857271906052395e-05,
      "loss": 0.4402,
      "step": 1020
    },
    {
      "epoch": 0.9304426377597109,
      "grad_norm": 3.420012950897217,
      "learning_rate": 1.3797049081601927e-05,
      "loss": 0.3924,
      "step": 1030
    },
    {
      "epoch": 0.9394760614272809,
      "grad_norm": 3.7430927753448486,
      "learning_rate": 1.373682625715146e-05,
      "loss": 0.3079,
      "step": 1040
    },
    {
      "epoch": 0.948509485094851,
      "grad_norm": 12.873998641967773,
      "learning_rate": 1.3676603432700995e-05,
      "loss": 0.5964,
      "step": 1050
    },
    {
      "epoch": 0.957542908762421,
      "grad_norm": 10.984349250793457,
      "learning_rate": 1.3616380608250528e-05,
      "loss": 0.5321,
      "step": 1060
    },
    {
      "epoch": 0.966576332429991,
      "grad_norm": 5.353321075439453,
      "learning_rate": 1.355615778380006e-05,
      "loss": 0.5147,
      "step": 1070
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 6.292019367218018,
      "learning_rate": 1.3495934959349594e-05,
      "loss": 0.4645,
      "step": 1080
    },
    {
      "epoch": 0.9846431797651309,
      "grad_norm": 5.454163074493408,
      "learning_rate": 1.3435712134899128e-05,
      "loss": 0.5185,
      "step": 1090
    },
    {
      "epoch": 0.993676603432701,
      "grad_norm": 13.36801528930664,
      "learning_rate": 1.3375489310448662e-05,
      "loss": 0.4716,
      "step": 1100
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7881662149954833,
      "eval_f1": 0.8021713312155341,
      "eval_loss": 0.4270123243331909,
      "eval_precision": 0.8488633434975786,
      "eval_recall": 0.7881662149954833,
      "eval_runtime": 11.1673,
      "eval_samples_per_second": 396.514,
      "eval_steps_per_second": 24.805,
      "step": 1107
    },
    {
      "epoch": 1.002710027100271,
      "grad_norm": 5.097996711730957,
      "learning_rate": 1.3315266485998196e-05,
      "loss": 0.425,
      "step": 1110
    },
    {
      "epoch": 1.0117434507678411,
      "grad_norm": 4.297029972076416,
      "learning_rate": 1.3255043661547728e-05,
      "loss": 0.3988,
      "step": 1120
    },
    {
      "epoch": 1.020776874435411,
      "grad_norm": 6.63625955581665,
      "learning_rate": 1.3194820837097262e-05,
      "loss": 0.297,
      "step": 1130
    },
    {
      "epoch": 1.029810298102981,
      "grad_norm": 7.488468647003174,
      "learning_rate": 1.3134598012646794e-05,
      "loss": 0.3872,
      "step": 1140
    },
    {
      "epoch": 1.038843721770551,
      "grad_norm": 3.684342622756958,
      "learning_rate": 1.3074375188196326e-05,
      "loss": 0.3196,
      "step": 1150
    },
    {
      "epoch": 1.047877145438121,
      "grad_norm": 9.29209041595459,
      "learning_rate": 1.301415236374586e-05,
      "loss": 0.4541,
      "step": 1160
    },
    {
      "epoch": 1.056910569105691,
      "grad_norm": 7.156675815582275,
      "learning_rate": 1.2953929539295394e-05,
      "loss": 0.3217,
      "step": 1170
    },
    {
      "epoch": 1.065943992773261,
      "grad_norm": 3.8980538845062256,
      "learning_rate": 1.2893706714844928e-05,
      "loss": 0.3418,
      "step": 1180
    },
    {
      "epoch": 1.074977416440831,
      "grad_norm": 5.066124439239502,
      "learning_rate": 1.283348389039446e-05,
      "loss": 0.3716,
      "step": 1190
    },
    {
      "epoch": 1.084010840108401,
      "grad_norm": 8.884013175964355,
      "learning_rate": 1.2773261065943994e-05,
      "loss": 0.3686,
      "step": 1200
    },
    {
      "epoch": 1.0930442637759712,
      "grad_norm": 10.737432479858398,
      "learning_rate": 1.2713038241493527e-05,
      "loss": 0.301,
      "step": 1210
    },
    {
      "epoch": 1.1020776874435412,
      "grad_norm": 10.033275604248047,
      "learning_rate": 1.2652815417043061e-05,
      "loss": 0.482,
      "step": 1220
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 18.788856506347656,
      "learning_rate": 1.2592592592592593e-05,
      "loss": 0.3378,
      "step": 1230
    },
    {
      "epoch": 1.1201445347786811,
      "grad_norm": 3.110642671585083,
      "learning_rate": 1.2532369768142127e-05,
      "loss": 0.5813,
      "step": 1240
    },
    {
      "epoch": 1.1291779584462511,
      "grad_norm": 8.596647262573242,
      "learning_rate": 1.2472146943691661e-05,
      "loss": 0.3292,
      "step": 1250
    },
    {
      "epoch": 1.1382113821138211,
      "grad_norm": 4.869574069976807,
      "learning_rate": 1.2411924119241195e-05,
      "loss": 0.3325,
      "step": 1260
    },
    {
      "epoch": 1.147244805781391,
      "grad_norm": 5.2981181144714355,
      "learning_rate": 1.2351701294790727e-05,
      "loss": 0.3209,
      "step": 1270
    },
    {
      "epoch": 1.156278229448961,
      "grad_norm": 3.2099382877349854,
      "learning_rate": 1.2291478470340259e-05,
      "loss": 0.3181,
      "step": 1280
    },
    {
      "epoch": 1.165311653116531,
      "grad_norm": 3.1180644035339355,
      "learning_rate": 1.2231255645889793e-05,
      "loss": 0.3001,
      "step": 1290
    },
    {
      "epoch": 1.174345076784101,
      "grad_norm": 5.405179500579834,
      "learning_rate": 1.2171032821439325e-05,
      "loss": 0.4425,
      "step": 1300
    },
    {
      "epoch": 1.1833785004516713,
      "grad_norm": 11.265365600585938,
      "learning_rate": 1.2110809996988859e-05,
      "loss": 0.3829,
      "step": 1310
    },
    {
      "epoch": 1.1924119241192412,
      "grad_norm": 7.284879684448242,
      "learning_rate": 1.2050587172538393e-05,
      "loss": 0.4097,
      "step": 1320
    },
    {
      "epoch": 1.2014453477868112,
      "grad_norm": 8.842948913574219,
      "learning_rate": 1.1990364348087927e-05,
      "loss": 0.3169,
      "step": 1330
    },
    {
      "epoch": 1.2104787714543812,
      "grad_norm": 9.0170316696167,
      "learning_rate": 1.193014152363746e-05,
      "loss": 0.3664,
      "step": 1340
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 6.3950700759887695,
      "learning_rate": 1.1869918699186992e-05,
      "loss": 0.2542,
      "step": 1350
    },
    {
      "epoch": 1.2285456187895212,
      "grad_norm": 9.270143508911133,
      "learning_rate": 1.1809695874736526e-05,
      "loss": 0.5189,
      "step": 1360
    },
    {
      "epoch": 1.2375790424570912,
      "grad_norm": 6.059751033782959,
      "learning_rate": 1.174947305028606e-05,
      "loss": 0.3299,
      "step": 1370
    },
    {
      "epoch": 1.2466124661246614,
      "grad_norm": 21.44864273071289,
      "learning_rate": 1.1689250225835594e-05,
      "loss": 0.3795,
      "step": 1380
    },
    {
      "epoch": 1.2556458897922314,
      "grad_norm": 4.78690767288208,
      "learning_rate": 1.1629027401385126e-05,
      "loss": 0.3823,
      "step": 1390
    },
    {
      "epoch": 1.2646793134598013,
      "grad_norm": 7.072218418121338,
      "learning_rate": 1.156880457693466e-05,
      "loss": 0.2176,
      "step": 1400
    },
    {
      "epoch": 1.2737127371273713,
      "grad_norm": 6.962742805480957,
      "learning_rate": 1.1508581752484192e-05,
      "loss": 0.2899,
      "step": 1410
    },
    {
      "epoch": 1.2827461607949413,
      "grad_norm": 8.996200561523438,
      "learning_rate": 1.1448358928033724e-05,
      "loss": 0.4866,
      "step": 1420
    },
    {
      "epoch": 1.2917795844625113,
      "grad_norm": 8.585345268249512,
      "learning_rate": 1.1388136103583258e-05,
      "loss": 0.2893,
      "step": 1430
    },
    {
      "epoch": 1.3008130081300813,
      "grad_norm": 5.550473213195801,
      "learning_rate": 1.1327913279132792e-05,
      "loss": 0.2676,
      "step": 1440
    },
    {
      "epoch": 1.3098464317976513,
      "grad_norm": 9.005120277404785,
      "learning_rate": 1.1267690454682326e-05,
      "loss": 0.3868,
      "step": 1450
    },
    {
      "epoch": 1.3188798554652212,
      "grad_norm": 11.801639556884766,
      "learning_rate": 1.1207467630231858e-05,
      "loss": 0.3617,
      "step": 1460
    },
    {
      "epoch": 1.3279132791327912,
      "grad_norm": 7.096296787261963,
      "learning_rate": 1.1147244805781392e-05,
      "loss": 0.3432,
      "step": 1470
    },
    {
      "epoch": 1.3369467028003612,
      "grad_norm": 5.235444068908691,
      "learning_rate": 1.1087021981330926e-05,
      "loss": 0.2992,
      "step": 1480
    },
    {
      "epoch": 1.3459801264679314,
      "grad_norm": 3.8607397079467773,
      "learning_rate": 1.102679915688046e-05,
      "loss": 0.2879,
      "step": 1490
    },
    {
      "epoch": 1.3550135501355014,
      "grad_norm": 3.6093740463256836,
      "learning_rate": 1.0966576332429991e-05,
      "loss": 0.3737,
      "step": 1500
    },
    {
      "epoch": 1.3640469738030714,
      "grad_norm": 10.071091651916504,
      "learning_rate": 1.0906353507979525e-05,
      "loss": 0.3735,
      "step": 1510
    },
    {
      "epoch": 1.3730803974706414,
      "grad_norm": 9.436301231384277,
      "learning_rate": 1.0846130683529059e-05,
      "loss": 0.3089,
      "step": 1520
    },
    {
      "epoch": 1.3821138211382114,
      "grad_norm": 2.1864824295043945,
      "learning_rate": 1.0785907859078593e-05,
      "loss": 0.3842,
      "step": 1530
    },
    {
      "epoch": 1.3911472448057813,
      "grad_norm": 4.118549346923828,
      "learning_rate": 1.0725685034628123e-05,
      "loss": 0.3395,
      "step": 1540
    },
    {
      "epoch": 1.4001806684733513,
      "grad_norm": 9.173748016357422,
      "learning_rate": 1.0665462210177657e-05,
      "loss": 0.2632,
      "step": 1550
    },
    {
      "epoch": 1.4092140921409215,
      "grad_norm": 10.200075149536133,
      "learning_rate": 1.0605239385727191e-05,
      "loss": 0.3927,
      "step": 1560
    },
    {
      "epoch": 1.4182475158084915,
      "grad_norm": 2.907257556915283,
      "learning_rate": 1.0545016561276725e-05,
      "loss": 0.4183,
      "step": 1570
    },
    {
      "epoch": 1.4272809394760615,
      "grad_norm": 11.117387771606445,
      "learning_rate": 1.0484793736826257e-05,
      "loss": 0.2722,
      "step": 1580
    },
    {
      "epoch": 1.4363143631436315,
      "grad_norm": 6.5450944900512695,
      "learning_rate": 1.0424570912375791e-05,
      "loss": 0.3394,
      "step": 1590
    },
    {
      "epoch": 1.4453477868112015,
      "grad_norm": 22.088001251220703,
      "learning_rate": 1.0364348087925325e-05,
      "loss": 0.3908,
      "step": 1600
    },
    {
      "epoch": 1.4543812104787714,
      "grad_norm": 12.904899597167969,
      "learning_rate": 1.0304125263474859e-05,
      "loss": 0.3633,
      "step": 1610
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 7.001129150390625,
      "learning_rate": 1.024390243902439e-05,
      "loss": 0.3474,
      "step": 1620
    },
    {
      "epoch": 1.4724480578139114,
      "grad_norm": 13.921147346496582,
      "learning_rate": 1.0183679614573924e-05,
      "loss": 0.3148,
      "step": 1630
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 1.4567517042160034,
      "learning_rate": 1.0123456790123458e-05,
      "loss": 0.3377,
      "step": 1640
    },
    {
      "epoch": 1.4905149051490514,
      "grad_norm": 7.975962162017822,
      "learning_rate": 1.0063233965672992e-05,
      "loss": 0.3023,
      "step": 1650
    },
    {
      "epoch": 1.4995483288166214,
      "grad_norm": 2.4276328086853027,
      "learning_rate": 1.0003011141222524e-05,
      "loss": 0.3174,
      "step": 1660
    },
    {
      "epoch": 1.5085817524841914,
      "grad_norm": 8.921131134033203,
      "learning_rate": 9.942788316772058e-06,
      "loss": 0.2981,
      "step": 1670
    },
    {
      "epoch": 1.5176151761517616,
      "grad_norm": 3.098360776901245,
      "learning_rate": 9.88256549232159e-06,
      "loss": 0.3184,
      "step": 1680
    },
    {
      "epoch": 1.5266485998193315,
      "grad_norm": 16.75661277770996,
      "learning_rate": 9.822342667871124e-06,
      "loss": 0.2828,
      "step": 1690
    },
    {
      "epoch": 1.5356820234869015,
      "grad_norm": 9.920574188232422,
      "learning_rate": 9.762119843420658e-06,
      "loss": 0.3996,
      "step": 1700
    },
    {
      "epoch": 1.5447154471544715,
      "grad_norm": 3.4504923820495605,
      "learning_rate": 9.70189701897019e-06,
      "loss": 0.3917,
      "step": 1710
    },
    {
      "epoch": 1.5537488708220417,
      "grad_norm": 7.375783443450928,
      "learning_rate": 9.641674194519724e-06,
      "loss": 0.3242,
      "step": 1720
    },
    {
      "epoch": 1.5627822944896117,
      "grad_norm": 7.883357048034668,
      "learning_rate": 9.581451370069256e-06,
      "loss": 0.2993,
      "step": 1730
    },
    {
      "epoch": 1.5718157181571817,
      "grad_norm": 29.759109497070312,
      "learning_rate": 9.52122854561879e-06,
      "loss": 0.3962,
      "step": 1740
    },
    {
      "epoch": 1.5808491418247517,
      "grad_norm": 6.820504665374756,
      "learning_rate": 9.461005721168324e-06,
      "loss": 0.2826,
      "step": 1750
    },
    {
      "epoch": 1.5898825654923217,
      "grad_norm": 8.289061546325684,
      "learning_rate": 9.400782896717858e-06,
      "loss": 0.4661,
      "step": 1760
    },
    {
      "epoch": 1.5989159891598916,
      "grad_norm": 6.054410457611084,
      "learning_rate": 9.340560072267391e-06,
      "loss": 0.3839,
      "step": 1770
    },
    {
      "epoch": 1.6079494128274616,
      "grad_norm": 11.13011646270752,
      "learning_rate": 9.280337247816923e-06,
      "loss": 0.3519,
      "step": 1780
    },
    {
      "epoch": 1.6169828364950316,
      "grad_norm": 11.056923866271973,
      "learning_rate": 9.220114423366456e-06,
      "loss": 0.3114,
      "step": 1790
    },
    {
      "epoch": 1.6260162601626016,
      "grad_norm": 7.8031792640686035,
      "learning_rate": 9.15989159891599e-06,
      "loss": 0.3781,
      "step": 1800
    },
    {
      "epoch": 1.6350496838301716,
      "grad_norm": 17.358917236328125,
      "learning_rate": 9.099668774465523e-06,
      "loss": 0.3437,
      "step": 1810
    },
    {
      "epoch": 1.6440831074977416,
      "grad_norm": 4.4932451248168945,
      "learning_rate": 9.039445950015057e-06,
      "loss": 0.2401,
      "step": 1820
    },
    {
      "epoch": 1.6531165311653115,
      "grad_norm": 12.263379096984863,
      "learning_rate": 8.97922312556459e-06,
      "loss": 0.3395,
      "step": 1830
    },
    {
      "epoch": 1.6621499548328815,
      "grad_norm": 19.344036102294922,
      "learning_rate": 8.919000301114123e-06,
      "loss": 0.4394,
      "step": 1840
    },
    {
      "epoch": 1.6711833785004515,
      "grad_norm": 6.693726539611816,
      "learning_rate": 8.858777476663655e-06,
      "loss": 0.3096,
      "step": 1850
    },
    {
      "epoch": 1.6802168021680217,
      "grad_norm": 15.306224822998047,
      "learning_rate": 8.798554652213189e-06,
      "loss": 0.306,
      "step": 1860
    },
    {
      "epoch": 1.6892502258355917,
      "grad_norm": 11.472643852233887,
      "learning_rate": 8.738331827762723e-06,
      "loss": 0.417,
      "step": 1870
    },
    {
      "epoch": 1.6982836495031617,
      "grad_norm": 6.945302963256836,
      "learning_rate": 8.678109003312257e-06,
      "loss": 0.2101,
      "step": 1880
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 3.7640910148620605,
      "learning_rate": 8.617886178861789e-06,
      "loss": 0.2074,
      "step": 1890
    },
    {
      "epoch": 1.7163504968383019,
      "grad_norm": 13.38819694519043,
      "learning_rate": 8.557663354411323e-06,
      "loss": 0.4047,
      "step": 1900
    },
    {
      "epoch": 1.7253839205058719,
      "grad_norm": 9.522209167480469,
      "learning_rate": 8.497440529960855e-06,
      "loss": 0.2642,
      "step": 1910
    },
    {
      "epoch": 1.7344173441734418,
      "grad_norm": 15.822439193725586,
      "learning_rate": 8.437217705510389e-06,
      "loss": 0.2601,
      "step": 1920
    },
    {
      "epoch": 1.7434507678410118,
      "grad_norm": 1.3416608572006226,
      "learning_rate": 8.376994881059922e-06,
      "loss": 0.4446,
      "step": 1930
    },
    {
      "epoch": 1.7524841915085818,
      "grad_norm": 18.119918823242188,
      "learning_rate": 8.316772056609456e-06,
      "loss": 0.4036,
      "step": 1940
    },
    {
      "epoch": 1.7615176151761518,
      "grad_norm": 5.910003662109375,
      "learning_rate": 8.256549232158988e-06,
      "loss": 0.2121,
      "step": 1950
    },
    {
      "epoch": 1.7705510388437218,
      "grad_norm": 3.8922877311706543,
      "learning_rate": 8.196326407708522e-06,
      "loss": 0.2616,
      "step": 1960
    },
    {
      "epoch": 1.7795844625112918,
      "grad_norm": 11.183692932128906,
      "learning_rate": 8.136103583258056e-06,
      "loss": 0.3632,
      "step": 1970
    },
    {
      "epoch": 1.7886178861788617,
      "grad_norm": 5.0584845542907715,
      "learning_rate": 8.075880758807588e-06,
      "loss": 0.2998,
      "step": 1980
    },
    {
      "epoch": 1.7976513098464317,
      "grad_norm": 11.60136890411377,
      "learning_rate": 8.015657934357122e-06,
      "loss": 0.3242,
      "step": 1990
    },
    {
      "epoch": 1.8066847335140017,
      "grad_norm": 17.369731903076172,
      "learning_rate": 7.955435109906656e-06,
      "loss": 0.3902,
      "step": 2000
    },
    {
      "epoch": 1.8157181571815717,
      "grad_norm": 5.744047164916992,
      "learning_rate": 7.895212285456188e-06,
      "loss": 0.2996,
      "step": 2010
    },
    {
      "epoch": 1.8247515808491417,
      "grad_norm": 12.03867244720459,
      "learning_rate": 7.834989461005722e-06,
      "loss": 0.3231,
      "step": 2020
    },
    {
      "epoch": 1.8337850045167117,
      "grad_norm": 4.617316246032715,
      "learning_rate": 7.774766636555256e-06,
      "loss": 0.3778,
      "step": 2030
    },
    {
      "epoch": 1.8428184281842819,
      "grad_norm": 10.927779197692871,
      "learning_rate": 7.71454381210479e-06,
      "loss": 0.3454,
      "step": 2040
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 13.501697540283203,
      "learning_rate": 7.654320987654322e-06,
      "loss": 0.3961,
      "step": 2050
    },
    {
      "epoch": 1.8608852755194218,
      "grad_norm": 4.487914562225342,
      "learning_rate": 7.594098163203855e-06,
      "loss": 0.3352,
      "step": 2060
    },
    {
      "epoch": 1.8699186991869918,
      "grad_norm": 8.7145414352417,
      "learning_rate": 7.5338753387533885e-06,
      "loss": 0.3093,
      "step": 2070
    },
    {
      "epoch": 1.878952122854562,
      "grad_norm": 8.83001708984375,
      "learning_rate": 7.4736525143029215e-06,
      "loss": 0.2283,
      "step": 2080
    },
    {
      "epoch": 1.887985546522132,
      "grad_norm": 15.895772933959961,
      "learning_rate": 7.413429689852455e-06,
      "loss": 0.3633,
      "step": 2090
    },
    {
      "epoch": 1.897018970189702,
      "grad_norm": 11.106071472167969,
      "learning_rate": 7.353206865401988e-06,
      "loss": 0.3253,
      "step": 2100
    },
    {
      "epoch": 1.906052393857272,
      "grad_norm": 10.002336502075195,
      "learning_rate": 7.29298404095152e-06,
      "loss": 0.2298,
      "step": 2110
    },
    {
      "epoch": 1.915085817524842,
      "grad_norm": 17.045766830444336,
      "learning_rate": 7.232761216501054e-06,
      "loss": 0.425,
      "step": 2120
    },
    {
      "epoch": 1.924119241192412,
      "grad_norm": 7.9117231369018555,
      "learning_rate": 7.172538392050587e-06,
      "loss": 0.3828,
      "step": 2130
    },
    {
      "epoch": 1.933152664859982,
      "grad_norm": 4.61888313293457,
      "learning_rate": 7.112315567600121e-06,
      "loss": 0.3728,
      "step": 2140
    },
    {
      "epoch": 1.942186088527552,
      "grad_norm": 10.190594673156738,
      "learning_rate": 7.052092743149654e-06,
      "loss": 0.3143,
      "step": 2150
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 17.379995346069336,
      "learning_rate": 6.991869918699188e-06,
      "loss": 0.3208,
      "step": 2160
    },
    {
      "epoch": 1.960252935862692,
      "grad_norm": 5.910555839538574,
      "learning_rate": 6.931647094248722e-06,
      "loss": 0.3443,
      "step": 2170
    },
    {
      "epoch": 1.9692863595302619,
      "grad_norm": 7.494762420654297,
      "learning_rate": 6.871424269798254e-06,
      "loss": 0.2738,
      "step": 2180
    },
    {
      "epoch": 1.9783197831978319,
      "grad_norm": 7.803838729858398,
      "learning_rate": 6.811201445347787e-06,
      "loss": 0.4282,
      "step": 2190
    },
    {
      "epoch": 1.9873532068654018,
      "grad_norm": 11.705406188964844,
      "learning_rate": 6.750978620897321e-06,
      "loss": 0.3632,
      "step": 2200
    },
    {
      "epoch": 1.996386630532972,
      "grad_norm": 14.946836471557617,
      "learning_rate": 6.690755796446854e-06,
      "loss": 0.2875,
      "step": 2210
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8448509485094851,
      "eval_f1": 0.850287284342855,
      "eval_loss": 0.43149876594543457,
      "eval_precision": 0.8617343814575903,
      "eval_recall": 0.8448509485094851,
      "eval_runtime": 10.938,
      "eval_samples_per_second": 404.829,
      "eval_steps_per_second": 25.325,
      "step": 2214
    },
    {
      "epoch": 2.005420054200542,
      "grad_norm": 7.252138137817383,
      "learning_rate": 6.6305329719963875e-06,
      "loss": 0.1742,
      "step": 2220
    },
    {
      "epoch": 2.014453477868112,
      "grad_norm": 0.7929995656013489,
      "learning_rate": 6.5703101475459204e-06,
      "loss": 0.3072,
      "step": 2230
    },
    {
      "epoch": 2.0234869015356822,
      "grad_norm": 1.9982914924621582,
      "learning_rate": 6.510087323095454e-06,
      "loss": 0.1754,
      "step": 2240
    },
    {
      "epoch": 2.032520325203252,
      "grad_norm": 10.232318878173828,
      "learning_rate": 6.449864498644986e-06,
      "loss": 0.1273,
      "step": 2250
    },
    {
      "epoch": 2.041553748870822,
      "grad_norm": 6.787652015686035,
      "learning_rate": 6.38964167419452e-06,
      "loss": 0.2399,
      "step": 2260
    },
    {
      "epoch": 2.050587172538392,
      "grad_norm": 3.5699191093444824,
      "learning_rate": 6.329418849744053e-06,
      "loss": 0.2962,
      "step": 2270
    },
    {
      "epoch": 2.059620596205962,
      "grad_norm": 2.597580909729004,
      "learning_rate": 6.269196025293587e-06,
      "loss": 0.4228,
      "step": 2280
    },
    {
      "epoch": 2.068654019873532,
      "grad_norm": 11.118535995483398,
      "learning_rate": 6.20897320084312e-06,
      "loss": 0.3527,
      "step": 2290
    },
    {
      "epoch": 2.077687443541102,
      "grad_norm": 11.508502006530762,
      "learning_rate": 6.148750376392654e-06,
      "loss": 0.1995,
      "step": 2300
    },
    {
      "epoch": 2.086720867208672,
      "grad_norm": 10.061461448669434,
      "learning_rate": 6.088527551942186e-06,
      "loss": 0.2283,
      "step": 2310
    },
    {
      "epoch": 2.095754290876242,
      "grad_norm": 2.562221050262451,
      "learning_rate": 6.02830472749172e-06,
      "loss": 0.1712,
      "step": 2320
    },
    {
      "epoch": 2.104787714543812,
      "grad_norm": 15.201367378234863,
      "learning_rate": 5.968081903041253e-06,
      "loss": 0.3294,
      "step": 2330
    },
    {
      "epoch": 2.113821138211382,
      "grad_norm": 6.509984970092773,
      "learning_rate": 5.907859078590787e-06,
      "loss": 0.2074,
      "step": 2340
    },
    {
      "epoch": 2.122854561878952,
      "grad_norm": 12.456131935119629,
      "learning_rate": 5.84763625414032e-06,
      "loss": 0.3067,
      "step": 2350
    },
    {
      "epoch": 2.131887985546522,
      "grad_norm": 6.382546424865723,
      "learning_rate": 5.7874134296898535e-06,
      "loss": 0.1306,
      "step": 2360
    },
    {
      "epoch": 2.140921409214092,
      "grad_norm": 8.619534492492676,
      "learning_rate": 5.7271906052393864e-06,
      "loss": 0.1975,
      "step": 2370
    },
    {
      "epoch": 2.149954832881662,
      "grad_norm": 17.265151977539062,
      "learning_rate": 5.6669677807889186e-06,
      "loss": 0.2434,
      "step": 2380
    },
    {
      "epoch": 2.158988256549232,
      "grad_norm": 4.211785316467285,
      "learning_rate": 5.606744956338452e-06,
      "loss": 0.2979,
      "step": 2390
    },
    {
      "epoch": 2.168021680216802,
      "grad_norm": 14.142349243164062,
      "learning_rate": 5.546522131887986e-06,
      "loss": 0.2926,
      "step": 2400
    },
    {
      "epoch": 2.1770551038843724,
      "grad_norm": 11.359058380126953,
      "learning_rate": 5.486299307437519e-06,
      "loss": 0.3942,
      "step": 2410
    },
    {
      "epoch": 2.1860885275519424,
      "grad_norm": 7.079858779907227,
      "learning_rate": 5.426076482987053e-06,
      "loss": 0.2108,
      "step": 2420
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 4.023305892944336,
      "learning_rate": 5.365853658536586e-06,
      "loss": 0.1336,
      "step": 2430
    },
    {
      "epoch": 2.2041553748870824,
      "grad_norm": 31.337295532226562,
      "learning_rate": 5.30563083408612e-06,
      "loss": 0.1333,
      "step": 2440
    },
    {
      "epoch": 2.2131887985546523,
      "grad_norm": 23.24838638305664,
      "learning_rate": 5.245408009635652e-06,
      "loss": 0.2481,
      "step": 2450
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 2.782113790512085,
      "learning_rate": 5.185185185185185e-06,
      "loss": 0.1956,
      "step": 2460
    },
    {
      "epoch": 2.2312556458897923,
      "grad_norm": 7.429549217224121,
      "learning_rate": 5.124962360734719e-06,
      "loss": 0.2644,
      "step": 2470
    },
    {
      "epoch": 2.2402890695573623,
      "grad_norm": 3.6486544609069824,
      "learning_rate": 5.064739536284252e-06,
      "loss": 0.1534,
      "step": 2480
    },
    {
      "epoch": 2.2493224932249323,
      "grad_norm": 33.80369186401367,
      "learning_rate": 5.004516711833786e-06,
      "loss": 0.1765,
      "step": 2490
    },
    {
      "epoch": 2.2583559168925023,
      "grad_norm": 6.159808158874512,
      "learning_rate": 4.944293887383319e-06,
      "loss": 0.4271,
      "step": 2500
    },
    {
      "epoch": 2.2673893405600722,
      "grad_norm": 5.518901824951172,
      "learning_rate": 4.884071062932852e-06,
      "loss": 0.2563,
      "step": 2510
    },
    {
      "epoch": 2.2764227642276422,
      "grad_norm": 4.004624366760254,
      "learning_rate": 4.823848238482385e-06,
      "loss": 0.1362,
      "step": 2520
    },
    {
      "epoch": 2.285456187895212,
      "grad_norm": 7.64224100112915,
      "learning_rate": 4.763625414031918e-06,
      "loss": 0.2004,
      "step": 2530
    },
    {
      "epoch": 2.294489611562782,
      "grad_norm": 23.68170738220215,
      "learning_rate": 4.703402589581451e-06,
      "loss": 0.1967,
      "step": 2540
    },
    {
      "epoch": 2.303523035230352,
      "grad_norm": 6.068026065826416,
      "learning_rate": 4.643179765130985e-06,
      "loss": 0.3888,
      "step": 2550
    },
    {
      "epoch": 2.312556458897922,
      "grad_norm": 6.911498069763184,
      "learning_rate": 4.582956940680518e-06,
      "loss": 0.3601,
      "step": 2560
    },
    {
      "epoch": 2.321589882565492,
      "grad_norm": 19.315168380737305,
      "learning_rate": 4.522734116230051e-06,
      "loss": 0.3019,
      "step": 2570
    },
    {
      "epoch": 2.330623306233062,
      "grad_norm": 7.487086772918701,
      "learning_rate": 4.462511291779585e-06,
      "loss": 0.3066,
      "step": 2580
    },
    {
      "epoch": 2.339656729900632,
      "grad_norm": 8.787559509277344,
      "learning_rate": 4.402288467329118e-06,
      "loss": 0.2577,
      "step": 2590
    },
    {
      "epoch": 2.348690153568202,
      "grad_norm": 11.372540473937988,
      "learning_rate": 4.342065642878651e-06,
      "loss": 0.1714,
      "step": 2600
    },
    {
      "epoch": 2.3577235772357725,
      "grad_norm": 5.131263732910156,
      "learning_rate": 4.281842818428185e-06,
      "loss": 0.3853,
      "step": 2610
    },
    {
      "epoch": 2.3667570009033425,
      "grad_norm": 36.580345153808594,
      "learning_rate": 4.221619993977718e-06,
      "loss": 0.3053,
      "step": 2620
    },
    {
      "epoch": 2.3757904245709125,
      "grad_norm": 21.598901748657227,
      "learning_rate": 4.161397169527251e-06,
      "loss": 0.2223,
      "step": 2630
    },
    {
      "epoch": 2.3848238482384825,
      "grad_norm": 6.664691925048828,
      "learning_rate": 4.101174345076785e-06,
      "loss": 0.271,
      "step": 2640
    },
    {
      "epoch": 2.3938572719060525,
      "grad_norm": 15.260859489440918,
      "learning_rate": 4.040951520626318e-06,
      "loss": 0.1844,
      "step": 2650
    },
    {
      "epoch": 2.4028906955736224,
      "grad_norm": 18.85285186767578,
      "learning_rate": 3.9807286961758514e-06,
      "loss": 0.1979,
      "step": 2660
    },
    {
      "epoch": 2.4119241192411924,
      "grad_norm": 16.97743797302246,
      "learning_rate": 3.920505871725384e-06,
      "loss": 0.265,
      "step": 2670
    },
    {
      "epoch": 2.4209575429087624,
      "grad_norm": 4.788148403167725,
      "learning_rate": 3.860283047274917e-06,
      "loss": 0.199,
      "step": 2680
    },
    {
      "epoch": 2.4299909665763324,
      "grad_norm": 3.9497973918914795,
      "learning_rate": 3.800060222824451e-06,
      "loss": 0.181,
      "step": 2690
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 19.06055450439453,
      "learning_rate": 3.7398373983739838e-06,
      "loss": 0.203,
      "step": 2700
    },
    {
      "epoch": 2.4480578139114724,
      "grad_norm": 26.8592529296875,
      "learning_rate": 3.679614573923517e-06,
      "loss": 0.1681,
      "step": 2710
    },
    {
      "epoch": 2.4570912375790424,
      "grad_norm": 16.57231330871582,
      "learning_rate": 3.6193917494730506e-06,
      "loss": 0.3126,
      "step": 2720
    },
    {
      "epoch": 2.4661246612466123,
      "grad_norm": 18.62519073486328,
      "learning_rate": 3.5591689250225836e-06,
      "loss": 0.2135,
      "step": 2730
    },
    {
      "epoch": 2.4751580849141823,
      "grad_norm": 4.938520431518555,
      "learning_rate": 3.498946100572117e-06,
      "loss": 0.2823,
      "step": 2740
    },
    {
      "epoch": 2.4841915085817523,
      "grad_norm": 12.870896339416504,
      "learning_rate": 3.4387232761216504e-06,
      "loss": 0.2343,
      "step": 2750
    },
    {
      "epoch": 2.4932249322493227,
      "grad_norm": 24.561613082885742,
      "learning_rate": 3.378500451671184e-06,
      "loss": 0.3477,
      "step": 2760
    },
    {
      "epoch": 2.5022583559168927,
      "grad_norm": 7.504422664642334,
      "learning_rate": 3.3182776272207168e-06,
      "loss": 0.297,
      "step": 2770
    },
    {
      "epoch": 2.5112917795844627,
      "grad_norm": 7.28391695022583,
      "learning_rate": 3.25805480277025e-06,
      "loss": 0.3533,
      "step": 2780
    },
    {
      "epoch": 2.5203252032520327,
      "grad_norm": 30.190776824951172,
      "learning_rate": 3.1978319783197836e-06,
      "loss": 0.2845,
      "step": 2790
    },
    {
      "epoch": 2.5293586269196027,
      "grad_norm": 31.011938095092773,
      "learning_rate": 3.1376091538693166e-06,
      "loss": 0.2001,
      "step": 2800
    },
    {
      "epoch": 2.5383920505871727,
      "grad_norm": 26.759151458740234,
      "learning_rate": 3.07738632941885e-06,
      "loss": 0.3241,
      "step": 2810
    },
    {
      "epoch": 2.5474254742547426,
      "grad_norm": 7.811246395111084,
      "learning_rate": 3.0171635049683834e-06,
      "loss": 0.2496,
      "step": 2820
    },
    {
      "epoch": 2.5564588979223126,
      "grad_norm": 32.9625244140625,
      "learning_rate": 2.9569406805179164e-06,
      "loss": 0.2697,
      "step": 2830
    },
    {
      "epoch": 2.5654923215898826,
      "grad_norm": 30.73137092590332,
      "learning_rate": 2.8967178560674498e-06,
      "loss": 0.1567,
      "step": 2840
    },
    {
      "epoch": 2.5745257452574526,
      "grad_norm": 12.879167556762695,
      "learning_rate": 2.836495031616983e-06,
      "loss": 0.2329,
      "step": 2850
    },
    {
      "epoch": 2.5835591689250226,
      "grad_norm": 7.802986145019531,
      "learning_rate": 2.7762722071665166e-06,
      "loss": 0.1716,
      "step": 2860
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 6.763764381408691,
      "learning_rate": 2.7160493827160496e-06,
      "loss": 0.2987,
      "step": 2870
    },
    {
      "epoch": 2.6016260162601625,
      "grad_norm": 5.379096984863281,
      "learning_rate": 2.655826558265583e-06,
      "loss": 0.189,
      "step": 2880
    },
    {
      "epoch": 2.6106594399277325,
      "grad_norm": 19.86196517944336,
      "learning_rate": 2.5956037338151164e-06,
      "loss": 0.2455,
      "step": 2890
    },
    {
      "epoch": 2.6196928635953025,
      "grad_norm": 4.257846832275391,
      "learning_rate": 2.5353809093646494e-06,
      "loss": 0.2048,
      "step": 2900
    },
    {
      "epoch": 2.6287262872628725,
      "grad_norm": 17.494144439697266,
      "learning_rate": 2.4751580849141828e-06,
      "loss": 0.3235,
      "step": 2910
    },
    {
      "epoch": 2.6377597109304425,
      "grad_norm": 14.563311576843262,
      "learning_rate": 2.414935260463716e-06,
      "loss": 0.1923,
      "step": 2920
    },
    {
      "epoch": 2.6467931345980125,
      "grad_norm": 8.13202953338623,
      "learning_rate": 2.354712436013249e-06,
      "loss": 0.2797,
      "step": 2930
    },
    {
      "epoch": 2.6558265582655824,
      "grad_norm": 7.84553861618042,
      "learning_rate": 2.2944896115627826e-06,
      "loss": 0.2521,
      "step": 2940
    },
    {
      "epoch": 2.6648599819331524,
      "grad_norm": 10.94677448272705,
      "learning_rate": 2.234266787112316e-06,
      "loss": 0.1615,
      "step": 2950
    },
    {
      "epoch": 2.6738934056007224,
      "grad_norm": 3.435159683227539,
      "learning_rate": 2.174043962661849e-06,
      "loss": 0.2152,
      "step": 2960
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 11.185127258300781,
      "learning_rate": 2.1138211382113824e-06,
      "loss": 0.2159,
      "step": 2970
    },
    {
      "epoch": 2.691960252935863,
      "grad_norm": 18.564414978027344,
      "learning_rate": 2.0535983137609154e-06,
      "loss": 0.2536,
      "step": 2980
    },
    {
      "epoch": 2.700993676603433,
      "grad_norm": 3.503222703933716,
      "learning_rate": 1.9933754893104488e-06,
      "loss": 0.3229,
      "step": 2990
    },
    {
      "epoch": 2.710027100271003,
      "grad_norm": 7.674949645996094,
      "learning_rate": 1.933152664859982e-06,
      "loss": 0.1985,
      "step": 3000
    },
    {
      "epoch": 2.719060523938573,
      "grad_norm": 11.741189002990723,
      "learning_rate": 1.8729298404095154e-06,
      "loss": 0.2371,
      "step": 3010
    },
    {
      "epoch": 2.7280939476061428,
      "grad_norm": 2.463003158569336,
      "learning_rate": 1.8127070159590488e-06,
      "loss": 0.2624,
      "step": 3020
    },
    {
      "epoch": 2.7371273712737128,
      "grad_norm": 9.527994155883789,
      "learning_rate": 1.752484191508582e-06,
      "loss": 0.3204,
      "step": 3030
    },
    {
      "epoch": 2.7461607949412827,
      "grad_norm": 3.95521879196167,
      "learning_rate": 1.692261367058115e-06,
      "loss": 0.431,
      "step": 3040
    },
    {
      "epoch": 2.7551942186088527,
      "grad_norm": 10.484493255615234,
      "learning_rate": 1.6320385426076486e-06,
      "loss": 0.239,
      "step": 3050
    },
    {
      "epoch": 2.7642276422764227,
      "grad_norm": 6.4119343757629395,
      "learning_rate": 1.5718157181571816e-06,
      "loss": 0.2143,
      "step": 3060
    },
    {
      "epoch": 2.7732610659439927,
      "grad_norm": 14.20169734954834,
      "learning_rate": 1.5115928937067152e-06,
      "loss": 0.2179,
      "step": 3070
    },
    {
      "epoch": 2.7822944896115627,
      "grad_norm": 3.3126816749572754,
      "learning_rate": 1.4513700692562482e-06,
      "loss": 0.0992,
      "step": 3080
    },
    {
      "epoch": 2.7913279132791327,
      "grad_norm": 9.498124122619629,
      "learning_rate": 1.3911472448057814e-06,
      "loss": 0.1609,
      "step": 3090
    },
    {
      "epoch": 2.8003613369467026,
      "grad_norm": 14.03175163269043,
      "learning_rate": 1.3309244203553148e-06,
      "loss": 0.2102,
      "step": 3100
    },
    {
      "epoch": 2.809394760614273,
      "grad_norm": 16.083904266357422,
      "learning_rate": 1.270701595904848e-06,
      "loss": 0.1453,
      "step": 3110
    },
    {
      "epoch": 2.818428184281843,
      "grad_norm": 10.638675689697266,
      "learning_rate": 1.2104787714543814e-06,
      "loss": 0.4076,
      "step": 3120
    },
    {
      "epoch": 2.827461607949413,
      "grad_norm": 5.730478763580322,
      "learning_rate": 1.1502559470039146e-06,
      "loss": 0.2061,
      "step": 3130
    },
    {
      "epoch": 2.836495031616983,
      "grad_norm": 1.0045024156570435,
      "learning_rate": 1.090033122553448e-06,
      "loss": 0.2237,
      "step": 3140
    },
    {
      "epoch": 2.845528455284553,
      "grad_norm": 8.524147033691406,
      "learning_rate": 1.0298102981029812e-06,
      "loss": 0.1146,
      "step": 3150
    },
    {
      "epoch": 2.854561878952123,
      "grad_norm": 19.658855438232422,
      "learning_rate": 9.695874736525144e-07,
      "loss": 0.1826,
      "step": 3160
    },
    {
      "epoch": 2.863595302619693,
      "grad_norm": 41.239871978759766,
      "learning_rate": 9.093646492020477e-07,
      "loss": 0.1773,
      "step": 3170
    },
    {
      "epoch": 2.872628726287263,
      "grad_norm": 14.105232238769531,
      "learning_rate": 8.49141824751581e-07,
      "loss": 0.2753,
      "step": 3180
    },
    {
      "epoch": 2.881662149954833,
      "grad_norm": 10.508086204528809,
      "learning_rate": 7.889190003011143e-07,
      "loss": 0.2455,
      "step": 3190
    },
    {
      "epoch": 2.890695573622403,
      "grad_norm": 19.421062469482422,
      "learning_rate": 7.286961758506475e-07,
      "loss": 0.1942,
      "step": 3200
    },
    {
      "epoch": 2.899728997289973,
      "grad_norm": 2.280437469482422,
      "learning_rate": 6.684733514001806e-07,
      "loss": 0.2,
      "step": 3210
    },
    {
      "epoch": 2.908762420957543,
      "grad_norm": 9.724849700927734,
      "learning_rate": 6.08250526949714e-07,
      "loss": 0.1939,
      "step": 3220
    },
    {
      "epoch": 2.917795844625113,
      "grad_norm": 22.999401092529297,
      "learning_rate": 5.480277024992472e-07,
      "loss": 0.2744,
      "step": 3230
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 23.75956153869629,
      "learning_rate": 4.878048780487805e-07,
      "loss": 0.4036,
      "step": 3240
    },
    {
      "epoch": 2.935862691960253,
      "grad_norm": 15.647298812866211,
      "learning_rate": 4.275820535983138e-07,
      "loss": 0.1412,
      "step": 3250
    },
    {
      "epoch": 2.944896115627823,
      "grad_norm": 18.466903686523438,
      "learning_rate": 3.6735922914784704e-07,
      "loss": 0.1786,
      "step": 3260
    },
    {
      "epoch": 2.953929539295393,
      "grad_norm": 0.329829603433609,
      "learning_rate": 3.0713640469738035e-07,
      "loss": 0.1731,
      "step": 3270
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 14.21483325958252,
      "learning_rate": 2.469135802469136e-07,
      "loss": 0.3522,
      "step": 3280
    },
    {
      "epoch": 2.971996386630533,
      "grad_norm": 15.333093643188477,
      "learning_rate": 1.866907557964469e-07,
      "loss": 0.3623,
      "step": 3290
    },
    {
      "epoch": 2.9810298102981028,
      "grad_norm": 10.398429870605469,
      "learning_rate": 1.2646793134598014e-07,
      "loss": 0.2695,
      "step": 3300
    },
    {
      "epoch": 2.9900632339656728,
      "grad_norm": 18.318164825439453,
      "learning_rate": 6.62451068955134e-08,
      "loss": 0.2718,
      "step": 3310
    },
    {
      "epoch": 2.9990966576332427,
      "grad_norm": 35.42837142944336,
      "learning_rate": 6.022282445046673e-09,
      "loss": 0.1439,
      "step": 3320
    }
  ],
  "logging_steps": 10,
  "max_steps": 3321,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1556441670202560.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
